{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import recommend\n",
    "from scipy import spatial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = load('lda_model.joblib') \n",
    "tf_vectorizer = load('tf_vectorizer.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_X = load('lda_X.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6587858708968571,\n",
       " 0.29186394289605827,\n",
       " 0.04873550877272852,\n",
       " 0.00030758496457447787,\n",
       " 0.00030709246978146077]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(lda_X[6])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vecs = lda_X[0:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_buttons = []\n",
    "for article_topics in topic_vecs:\n",
    "    topic_idx = list(article_topics.argsort()[::-1])\n",
    "    topic_sorted = sorted(article_topics)[::-1]\n",
    "\n",
    "    max_topic = topic_sorted[0]\n",
    "\n",
    "    if max_topic > .95:\n",
    "        strength = \"Strong\"\n",
    "    elif max_topic > .55 and max_topic < .95:\n",
    "        strength = \"Medium\"\n",
    "    else:\n",
    "        strength = \"Low\"\n",
    "    user_buttons.append(topic_idx[0:2]+ [strength])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "data model learning set models using algorithm used training neural based figure time results performance number problem function given control classification new mixture probability information approach class parameters method use tree decision network different 10 test networks space methods 1995 systems 1994 state recognition classifier linear local task vector value likelihood example reinforcement memory values experts input feature error algorithms table sample step 20 pp examples machine case features vectors et context experiments em 1993 best learned work processing search paper trees 1996 possible probabilities analysis density single problems al shown level real distance sets 12 non selection output estimation shows log classes statistical trained ii regression standard 100 robot order gaussian 1992 optimal representation estimates second expert prediction structure size high section large 11 simple bayesian points goal better obtained factor tasks university image matrix learn action current loss average process techniques distribution samples maximum procedure expected target cost rate estimate generated node following applied prior way functions pattern random policy described measure components small ieee dimensional point known similar initial continuous 16 form computer does 13 50 knowledge artificial press parameter good xi position let speech weighted independent approaches note variables available conditional component basis general computation accuracy\n",
      "Topic 1:\n",
      "function information model neural data matrix noise spike time vector learning 10 algorithm neurons linear given point gradient signal analysis independent component figure distribution functions field space order points case neuron mean input results using rate 1995 natural form non activity components entropy networks network number used et 1996 source set density dimensional ii shown al different rule fig eq dynamics theory local firing zero 1997 models parameter phase single fixed signals systems new pattern weight correlation example ai gaussian xi line map method based probability maximum approach motor threshold patterns conditions problem use synaptic approximation equation constant response assume 11 00 following stimulus processing terms right high vectors stimuli solution structure parameters 1994 positive simple statistical distributions large left general defined 14 small temporal second random corresponding cell frequency section computational basis range estimate train term value let size equations possible cortical effect values paper nonlinear low limit distance variance self consider cortex computation image optimization properties mapping unit note does xl inputs work loss methods obtained variables stochastic 12 similar state scale observed shows negative 1993 variable output process average algorithms statistics higher particular weights changes press result constraints 20 assumption potential behavior spatial eye estimation minimum feature\n",
      "Topic 2:\n",
      "model figure input visual cells time cell neural image network neurons output object circuit frequency neuron response learning motion images different orientation used shown direction information using temporal processing stimulus results recognition connections activity current cortex spatial set 10 analog et order field pattern eye function based target fig filter signal face single al layer data inputs shows responses 20 objects synaptic position right receptive filters second head feature 1995 voltage performance contrast phase systems cortical local left similar low simple high features center networks representation human number chip models properties complex units analysis view signals detection control threshold stimuli feedback 1994 non use constant range gain noise rate line value case training small given 1993 level firing 40 linear presented effect 1996 task 1991 present 1992 changes 60 example experiments 100 work 12 11 potential 15 30 point vi parallel space higher weights scale described representations computational patterns values behavior map like ii large particular pp long test result obtained 1989 spike process size structure change simulation architecture observed curves 50 1990 type computation elements computer experimental research 80 curve weight 00 relative possible adaptation parameters university memory natural does press unit real self power average stage outputs distribution\n",
      "Topic 3:\n",
      "network neural input units networks time training output hidden figure unit using used layer set error data learning performance weights recognition number information speech patterns results 10 systems different state algorithm problem inputs trained recurrent word test function shown weight pattern architecture use values rate task vector words linear processing method signal control outputs activation second et single net given non memory based approach model table shows propagation order solution 20 al channel possible real connections sequence ii large new node case fixed experiments features small value 16 noise example described feature methods section adaptive paper presented problems nonlinear energy better 100 high adaptation point initial classification simple term feedback 1994 representations random 1988 learn 1992 long average individual work internal research prediction current parameters step matrix continuous 12 local motion class similar parallel stage previous standard 1987 following 1986 correlation nodes size rates applied models target analysis represent map zero level frequency machine examples probability series mean human space required 1989 university changes present 11 behavior chip dynamics ieee obtained 1995 errors available proposed sets form corresponding computation 1993 signals transition hand generalization 50 vectors does produce low active process performed difference particular representation gradient binary computer experiment like\n",
      "Topic 4:\n",
      "learning function networks set algorithm training error state network neural distribution data value number given problem using input optimal case figure results probability functions used model parameters linear space weights time mean output generalization values gaussian order hidden noise variables use 10 approximation examples algorithms weight problems let method vector process paper example point convergence matrix following 1995 different bound large step states form new parameter consider size points equation local finite variance information line based rate 11 1996 possible markov theorem bayesian section theory small approach shown inputs xi terms performance random note result regression stochastic methods term gradient policy zero continuous class defined iteration solution machine cost fixed models simple test average log ii ensemble obtained units bounds update prediction general 1994 et estimate vectors al equations shows assume wi nodes standard structure xt corresponding graph likelihood 12 prior single sample dimension university analysis computation validation non bias representation 00 constant systems initial sets work field respect energy posterior variable basis expected second procedure cases optimization dynamics layer particular does node follows chosen 1993 similar 20 transition 100 statistical lower conditional define obtain decision true sampling better minimum programming descent real action known distributions upper sequence dynamic covariance\n"
     ]
    }
   ],
   "source": [
    "# get the topics\n",
    "display_topics(lda, tf_vectorizer.get_feature_names(), 200)\n",
    "## Topic 0: \n",
    "## Topic 1: Theoretical and computational neuroscience \n",
    "## spke train, signal analysis, entropy networks, dynamics theory, firing, synaptic, \n",
    "## Topic 2: Interdisciplinary work between experimental neuroscience and machine learning \n",
    "## Stimuli, Visual cells, neurons, direction/spatial selectivity, synaptic,  receptive filters, voltage, cortex, \n",
    "## Topic 3: Neural networks applications paper\n",
    "## Keywords: Hidden units, image recognition, speech recognition, recurrent neural networks, artchitecture, \n",
    "## Topic 4: Bayesian machine learning and theoretical probability theory and statistics\n",
    "## Keywords: Markov Chain, Hidden Markov Models, Latent Variable Models, Bayesian theory, Stochastic methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
