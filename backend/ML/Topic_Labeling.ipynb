{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "import recommend\n",
    "from scipy import spatial\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function for displaying the top words for a topic \n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load in models and data\n",
    "lda = load('lda_model.joblib') \n",
    "tf_vectorizer = load('tf_vectorizer.joblib')\n",
    "lda_X = load('lda_X.joblib')\n",
    "papers = pd.read_csv('papers.csv')\n",
    "authors = pd.read_csv('authors.csv')\n",
    "paper_authors = pd.read_csv('paper_authors.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "data model learning set models using algorithm used training neural based figure time results performance number problem function given control classification new mixture probability information approach class parameters method use tree decision network different 10 test networks space methods 1995 systems 1994 state recognition classifier linear local task vector value likelihood example reinforcement memory values experts input feature error algorithms table sample step 20 pp examples machine case features vectors et context experiments em 1993 best learned work processing search paper trees 1996 possible probabilities analysis density single problems al shown level real distance sets 12 non selection output estimation shows log classes statistical trained ii regression standard 100 robot order gaussian 1992 optimal representation estimates second expert prediction structure size high section large 11 simple bayesian points goal better obtained factor tasks university image matrix learn action current loss average process techniques distribution samples maximum procedure expected target cost\n",
      "Topic 1:\n",
      "function information model neural data matrix noise spike time vector learning 10 algorithm neurons linear given point gradient signal analysis independent component figure distribution functions field space order points case neuron mean input results using rate 1995 natural form non activity components entropy networks network number used et 1996 source set density dimensional ii shown al different rule fig eq dynamics theory local firing zero 1997 models parameter phase single fixed signals systems new pattern weight correlation example ai gaussian xi line map method based probability maximum approach motor threshold patterns conditions problem use synaptic approximation equation constant response assume 11 00 following stimulus processing terms right high vectors stimuli solution structure parameters 1994 positive simple statistical distributions large left general defined 14 small temporal second random corresponding cell frequency section computational basis range estimate train term value let size equations possible cortical effect values paper nonlinear low limit distance\n",
      "Topic 2:\n",
      "model figure input visual cells time cell neural image network neurons output object circuit frequency neuron response learning motion images different orientation used shown direction information using temporal processing stimulus results recognition connections activity current cortex spatial set 10 analog et order field pattern eye function based target fig filter signal face single al layer data inputs shows responses 20 objects synaptic position right receptive filters second head feature 1995 voltage performance contrast phase systems cortical local left similar low simple high features center networks representation human number chip models properties complex units analysis view signals detection control threshold stimuli feedback 1994 non use constant range gain noise rate line value case training small given 1993 level firing 40 linear presented effect 1996 task 1991 present 1992 changes 60 example experiments 100 work 12 11 potential 15 30 point vi parallel space higher weights scale described representations computational patterns values\n",
      "Topic 3:\n",
      "network neural input units networks time training output hidden figure unit using used layer set error data learning performance weights recognition number information speech patterns results 10 systems different state algorithm problem inputs trained recurrent word test function shown weight pattern architecture use values rate task vector words linear processing method signal control outputs activation second et single net given non memory based approach model table shows propagation order solution 20 al channel possible real connections sequence ii large new node case fixed experiments features small value 16 noise example described feature methods section adaptive paper presented problems nonlinear energy better 100 high adaptation point initial classification simple term feedback 1994 representations random 1988 learn 1992 long average individual work internal research prediction current parameters step matrix continuous 12 local motion class similar parallel stage previous standard 1987 following 1986 correlation nodes size rates applied models target analysis represent map\n",
      "Topic 4:\n",
      "learning function networks set algorithm training error state network neural distribution data value number given problem using input optimal case figure results probability functions used model parameters linear space weights time mean output generalization values gaussian order hidden noise variables use 10 approximation examples algorithms weight problems let method vector process paper example point convergence matrix following 1995 different bound large step states form new parameter consider size points equation local finite variance information line based rate 11 1996 possible markov theorem bayesian section theory small approach shown inputs xi terms performance random note result regression stochastic methods term gradient policy zero continuous class defined iteration solution machine cost fixed models simple test average log ii ensemble obtained units bounds update prediction general 1994 et estimate vectors al equations shows assume wi nodes standard structure xt corresponding graph likelihood 12 prior single sample dimension university analysis computation validation non bias\n"
     ]
    }
   ],
   "source": [
    "# get the topics\n",
    "display_topics(lda, tf_vectorizer.get_feature_names(), 150)\n",
    "## Topic 0: Applications\n",
    "## Topic 1: Computational neuroscience\n",
    "## spke train, signal analysis, entropy networks, dynamics theory, firing, synaptic, \n",
    "## Topic 2: Interdisciplinary work between experimental neuroscience and machine learning \n",
    "## Stimuli, Visual cells, neurons, direction/spatial selectivity, synaptic,  receptive filters, voltage, cortex, \n",
    "## Topic 3: Neural networks applications paper\n",
    "## Keywords: Hidden units, speech recognition,classification, recurrent neural networks, artchitecture, \n",
    "## Topic 4: Bayesian machine learning and theoretical probability theory and statistics\n",
    "## Keywords: Markov, Bayesian, chains, stochastic methods, theorem, regression, ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
